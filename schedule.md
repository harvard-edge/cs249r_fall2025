# Schedule & Readings

**Architecture 2.0: AI Agents for Computer Architecture ‚Ä¢ Fall 2025**

---

## Reading Schedule Overview

Each week explores how AI agents are transforming computer architecture - from code optimization and compiler design to chip placement and design space exploration. This is the story of how intelligent agents are becoming integral to every layer of computer architecture design.

**üìñ Reading Reflection Due**: Before each class session  
**üéØ Discussion Leadership**: Sign up by Week 3  
**üèóÔ∏è Architecture Matrix**: Mapping AI agents across the architecture stack

---

## Detailed Schedule

### **Week 1: Welcome to Architecture 2.0**
*September 2 & 4, 2025*

**Session 1 (Sep 2): The AI Agent Revolution in Architecture**
- Course vision: AI agents as architecture co-designers
- From heuristics to learned policies across the stack
- What makes this "Architecture 2.0"?

**Session 2 (Sep 4): Foundational Examples**
- **Required**: A graph placement methodology for fast chip design
- **Required**: Chip Placement with Deep Reinforcement Learning
- **Supplemental**: Placement Optimization with Deep Reinforcement Learning

---

### **Week 2: AI Agents in Design Space Exploration**
*September 9 & 11, 2025*

**Session 3 (Sep 9): ArchGym and Architecture Search**
- **Required**: ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design
- **Required**: A Learned Performance Model for Tensor Processing Units

**Session 4 (Sep 11): Reinforcement Learning for Architecture**
- **Required**: Reinforcement Learning: An Introduction (Chapters 1, 3, 6)
- **Required**: A Case for Efficient Accelerator Design Space Exploration via Bayesian Optimization

---

### **Week 3: AI Agents for Code Optimization**
*September 16 & 18, 2025*

**Session 5 (Sep 16): Compiler Optimization with RL**
- **Required**: CompilerGym: Robust, Performant Compiler Optimization Environments for AI Research
- **Required**: Learning to Optimize Tensor Programs

**Session 6 (Sep 18): Performance Prediction and Autotuning**
- **Required**: Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks
- **Required**: ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations

**üéØ Discussion Leadership Sign-ups Due**

---

### **Week 4: AI Agents for Memory Hierarchy Design**
*September 23 & 25, 2025*

**Session 7 (Sep 23): Cache Replacement and Prefetching**
- **Required**: Applying Deep Learning to the Cache Replacement Problem
- **Required**: Learning Memory Access Patterns

**Session 8 (Sep 25): Memory System Optimization**
- **Required**: Learning-based Memory Allocation for C++ Server Workloads
- **Required**: The Difference-Bit Cache

---

### **Week 5: AI Agents for Branch Prediction & Control Flow**
*September 30 & October 2, 2025*

**Session 9 (Sep 30): Neural Branch Prediction**
- **Required**: Neural Methods for Dynamic Branch Prediction
- **Required**: A case for (partially) TAgged GEometric history length branch prediction

**Session 10 (Oct 2): Control Flow and Speculation**
- **Required**: A Simple Model for Estimating Power-Performance Efficiency of Multicore Processors
- **Required**: TBD - Recent work on ML-based speculation

---

### **Week 6: Physical Design Automation with AI**
*October 7 & 9, 2025*

**Session 11 (Oct 7): DREAMPlace and GPU-Accelerated Placement**
- **Required**: DREAMPlace: Deep Learning Toolkit-Enabled GPU Acceleration for Modern VLSI Placement
- **Required**: DREAMPlace 2.0: Open-Source GPU-Accelerated Global and Detailed Placement for Large-Scale VLSI Designs

**Session 12 (Oct 9): Routing and Physical Synthesis**
- **Required**: The Routability-Driven Placement by Deep Reinforcement Learning
- **Required**: TBD - Recent routing optimization work

---

### **Week 7: AI Agents for Processor Microarchitecture**
*October 14 & 16, 2025*

**Session 13 (Oct 14): Superscalar Design and Scheduling**
- **Required**: TBD - ML-based instruction scheduling
- **Required**: Exploring the Potential of Heterogeneous Von Neumann/Dataflow Execution Models

**Session 14 (Oct 16): Out-of-Order Execution Optimization**
- **Required**: TBD - Recent work on ML for OoO optimization
- **Required**: MorphCore: An Energy-Efficient Microarchitecture for High Performance ILP and High Throughput TLP

---

### **Week 8: AI for Accelerator Design**
*October 21 & 23, 2025*

**Session 15 (Oct 21): Neural Architecture Search for Accelerators**
- **Required**: A Principled Approach to Accelerator Design Space Exploration
- **Required**: A Case for Efficient Accelerator Design Space Exploration via Bayesian Optimization

**Session 16 (Oct 23): Domain-Specific Architecture Generation**
- **Required**: TBD - Recent work on automated accelerator generation
- **Required**: Timeloop: A Systematic Approach to DNN Accelerator Evaluation

---

### **Week 9: AI for Interconnect and NoC Design**
*October 28 & 30, 2025*

**Session 17 (Oct 28): Network-on-Chip Optimization**
- **Required**: TBD - ML for NoC routing and topology
- **Required**: Route Packets, Not Wires: On-Chip Interconnection Networks

**Session 18 (Oct 30): Interconnect Design Space Exploration**
- **Required**: TBD - Recent work on AI for interconnect design
- **Required**: A 4.6Tbps 3.6GHz Single-cycle NoC Router with a Novel Switch Allocator in 65nm CMOS

---

### **Week 10: Advanced EDA with AI Agents**
*November 4 & 6, 2025*

**Session 19 (Nov 4): Logic Synthesis and Optimization**
- **Required**: TBD - Recent work on ML for logic synthesis
- **Required**: Developing Synthesis Flows without Human Knowledge

**Session 20 (Nov 6): Timing Analysis and Optimization**
- **Required**: TBD - ML for static timing analysis
- **Required**: ORION 2.0: A Fast and Accurate NoC Power and Area Model for Early-Stage Design Space Exploration

---

### **Week 11: AI for Power and Thermal Management**
*November 11 & 13, 2025*

**Session 21 (Nov 11): Dynamic Power Management**
- **Required**: TBD - ML-based power management policies
- **Required**: Thermal Modeling, Analysis, and Management in VLSI Circuits: Principles and Methods

**Session 22 (Nov 13): Thermal-Aware Design**
- **Required**: TBD - AI for thermal optimization
- **Required**: Temperature-Aware Microarchitecture

---

### **Week 12: Industry Perspectives & Future Directions**
*November 18 & 20, 2025*

**Session 23 (Nov 18): Industry Case Studies**
- **Guest Speaker**: Google/Intel/NVIDIA practitioner on production AI-for-architecture
- **Required**: Industry white papers (TBD based on speaker)

**Session 24 (Nov 20): The Future of Architecture 2.0**
- **Guest Speaker**: Leading researcher in AI-for-architecture
- **Required**: Vision papers and recent breakthrough work (TBD)

---

### **Week 13: Architecture 2.0 Research Landscape**
*November 25, 2025*

**Session 25 (Nov 25): Mapping the AI-Architecture Future**
- Collaborative matrix completion: Where are the gaps?
- Identifying the most promising research directions
- Preparing final position papers on Architecture 2.0

**November 27**: *No class - Thanksgiving Break*

---

### **Week 14: Student Visions for Architecture 2.0**
*December 2, 2025*

**Session 26 (Dec 2): Final Presentations**
- Student position paper presentations: "My Vision for Architecture 2.0"
- Course synthesis: What have we learned about AI agents in architecture?
- The road ahead: Research priorities and open challenges

**üìù Final Position Papers Due**

---

## Reading Access

- **Primary Sources**: Papers available through Harvard Library, ACM Digital Library, IEEE Xplore
- **Course Reserves**: Key papers uploaded to course management system
- **Supplemental Materials**: Additional resources linked from course website

## Discussion Format

Each session follows a structured format:
1. **Opening** (10 min): Key takeaways and questions from readings
2. **Deep Dive** (45 min): Structured discussion led by students or instructor  
3. **Matrix Mapping** (15 min): How do these papers fit in our framework?
4. **Synthesis** (5 min): Connections to previous sessions and upcoming topics

---

*Schedule subject to adjustment based on guest speaker availability and emerging research developments.*
