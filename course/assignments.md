---
layout: default
---

# Assignments & Deliverables

## Assignment Overview

This course emphasizes **active engagement** with cutting-edge research through three main components:

- **Survey Paper Checkpoints**: Three milestone checkpoints supporting the collaborative survey paper
- **Paper Presentations & Discussion Leadership**: Facilitating class discussions and synthesis
- **Group Survey Paper Project**: Collaborative research surveys advancing the field

---

## 1. Survey Paper Checkpoints (45%)

**Format**: Three milestone checkpoints (15% each)  
**Purpose**: Progressive development and validation of your survey paper research  
**Timeline**: Aligned with course phases and survey paper development

### Checkpoint Structure
- **Checkpoint 1**: Literature foundation and research planning
- **Checkpoint 2**: Data analysis and draft sections  
- **Checkpoint 3**: Integration and final paper preparation

Each checkpoint focuses on **research progress and quality** within the survey paper project, ensuring systematic development of publication-quality work.

*Detailed checkpoint specifications will be posted throughout the semester.*

---

## 2. Paper Presentations & Discussion Leadership (25%)

**Format**: Present and lead discussion for assigned papers (individually or in pairs)  
**Structure**: 20-minute presentation + 20-minute facilitated discussion per paper  
**Sign-up**: By Week 3

### Presentation Structure
**20-Minute Presentation:**
- Paper summary and key contributions (5-7 minutes)
- Critical analysis: strengths, limitations, methodology issues (8-10 minutes)
- Connections to course themes and other papers (3-5 minutes)

**20-Minute Discussion:**
- Facilitate Q&A and critical discussion
- Prepare 3-4 provocative discussion questions
- Guide synthesis and connections to broader themes

### Responsibilities
- **Preparation**: Meet with instructor 1 week before, prepare slides and discussion questions
- **Flexibility**: Timing may adjust based on guest speaker availability and class dynamics
- **Follow-up**: Brief reflection on presentation effectiveness and discussion insights

---

## 3. Collaborative Survey Paper Project (30%)

{: .course-goal}
> **üìã Complete Project Details**: For comprehensive information about the survey paper project, including detailed timeline, requirements, and examples, visit the dedicated **[Survey Project Page](project)**.

**Format**: Single comprehensive survey paper created collaboratively by all students  
**Scope**: "Agentic AI for Computer Systems Design: A Holistic Cross-Stack Perspective"  
**Goal**: Produce a publication-quality survey that synthesizes AI agent methodologies across the complete computing stack

### The Vision: Cross-Cutting Analysis

Rather than creating separate papers on individual topics, this class will produce **one comprehensive survey** that examines how AI agents are transforming computer systems design holistically‚Äîfrom software optimization to chip design. The value lies in identifying cross-cutting themes, methodological connections, and system-level insights that emerge when viewing these technologies as an integrated ecosystem.

{: .rationale-box}
### Why This Approach? The Value of Survey Research

**Research Skills Over Implementation**: Rather than coding assignments, this course emphasizes **research synthesis and critical analysis**‚Äîthe core skills needed to advance the field of AI agents for computer systems design.

**üîç Field-Shaping Impact**: The field of AI agents for computer systems is rapidly evolving with scattered research across venues (ISCA, MICRO, DAC, MLSys, ICLR, etc.). High-quality survey papers help consolidate knowledge, identify gaps, and guide future research directions.

**üìä Research Methodology**: Students develop crucial skills in systematic literature review, critical analysis, taxonomy development, gap identification, and technical writing for academic publication.

**üéØ Real Research Impact**: This survey is designed to be a **publication-quality** contribution suitable for top-tier venues like ISCA, MICRO, or Computer Architecture Letters‚Äîproviding the field's first comprehensive cross-stack analysis of agentic AI methodologies.

### Project Structure & Timeline

**Survey Paper Pipeline**: The project follows a structured pipeline that aligns with course learning phases, allowing students to research topics as they learn them in class.

<div class="mermaid">
flowchart TD
    A["Week 1-2: Course Setup<br/>Group Formation"] --> B["AI for Software Groups<br/>(2-3 students each)"]
    A --> C["AI for Architecture Groups<br/>(2-3 students each)"]
    A --> D["AI for Chip Design Groups<br/>(2-3 students each)"]
    
    B --> E["Weeks 2-5: Learn Software Topics<br/>+ Research Literature"]
    C --> F["Weeks 2-5: Learn Software Topics<br/>+ Plan Architecture Research"]
    D --> G["Weeks 2-5: Learn Software Topics<br/>+ Plan EDA Research"]
    
    E --> H["üìã Checkpoint 1: Oct 1<br/>Literature Review<br/>(Software Groups)"]
    
    F --> I["Weeks 6-9: Learn Architecture<br/>+ Research Literature"]
    G --> J["Weeks 6-9: Learn Architecture<br/>+ Plan EDA Research"]
    H --> K["Weeks 6-9: Continue Software<br/>Research & Writing"]
    
    I --> L["üìä Checkpoint 2: Oct 29<br/>Draft Sections<br/>(Architecture Groups)"]
    
    J --> M["Weeks 10-12: Learn EDA<br/>+ Research Literature"]
    K --> N["Weeks 10-12: Finalize Software<br/>Sections"]
    L --> O["Weeks 10-12: Finalize Architecture<br/>Sections"]
    
    M --> P["üìñ Checkpoint 3: Nov 19<br/>Final Integration<br/>(EDA Groups)"]
    
    N --> Q["Week 13: Final Assembly<br/>All Groups Integrate"]
    O --> Q
    P --> Q
    
    Q --> R["üéØ Dec 1: Final Presentations<br/>Complete Survey Papers"]
    
    style H fill:#e1f5fe
    style L fill:#f3e5f5
    style P fill:#e8f5e8
    style R fill:#fff3e0
</div>

{: .pipeline-diagram}
**Alternative Text View:**
```
Course Learning Flow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Survey Research Pipeline

Weeks 1-2: Group Formation    ‚îÄ‚îÄ‚Üí All Groups: Topic Selection & Approval
    ‚îÇ                               ‚îÇ
    ‚Üì                               ‚Üì
    
Weeks 2-5: AI for Software    ‚îÄ‚îÄ‚Üí üìã Checkpoint 1 (Oct 1): Software Groups
    ‚îÇ Learn & Discuss               ‚îÇ Literature Review (3-4 pages)
    ‚Üì                               ‚Üì
    
Weeks 6-9: AI for Architecture ‚îÄ‚îÄ‚Üí üìä Checkpoint 2 (Oct 29): Architecture Groups  
    ‚îÇ Learn & Discuss               ‚îÇ Draft Sections (8-10 pages)
    ‚Üì                               ‚Üì
    
Weeks 10-12: AI for EDA       ‚îÄ‚îÄ‚Üí üìñ Checkpoint 3 (Nov 19): EDA Groups
    ‚îÇ Learn & Discuss               ‚îÇ Final Papers (12-15 pages)
    ‚Üì                               ‚Üì
    
Week 13: Integration          ‚îÄ‚îÄ‚Üí üéØ Final Presentations (Dec 1): All Groups
```

### Survey Structure: Cross-Stack Sections (2-3 students per section)

**The paper will examine agentic AI methodologies across three abstraction levels, with emphasis on cross-cutting themes and system-level insights:**

**Section 1: AI Agents for Software Systems**
- Code generation, optimization, and performance engineering
- Cross-cutting theme: *Agent-driven software-hardware co-optimization*

**Section 2: AI Agents for Architecture Design**  
- Performance prediction, design space exploration, and accelerator design
- Cross-cutting theme: *Multi-objective optimization and design trade-offs*

**Section 3: AI Agents for Physical Implementation**
- RTL synthesis, placement & routing, and verification
- Cross-cutting theme: *Constraint satisfaction and manufacturing considerations*

**Section 4: Cross-Stack Integration & Future Directions**
- Methodological connections, API design, and holistic system design
- Cross-cutting theme: *End-to-end agentic design workflows*

**Section 5: Evaluation Frameworks & Benchmarking**
- Comparative analysis, metrics, and reproducibility
- Cross-cutting theme: *Standardized evaluation across abstraction levels*

### Data-Driven Analysis Requirements

**Every student must contribute quantitative analysis and data visualization:**

**Required Analytical Contributions (flexible based on section needs):**
- **Systematic data extraction** from papers, GitHub repos, benchmark leaderboards, and conference proceedings
- **Synthesis tables and figures** that reveal patterns not visible in individual papers
- **Trend analysis and timeline visualizations** showing technique evolution and adoption
- **Performance landscape mapping** using aggregated results from multiple sources
- **Gap identification** through systematic coverage analysis of evaluation spaces

**Technical Work Guidelines:**
- **Data-driven approach**: Every major claim supported by tables, figures, or analysis
- **Code for analysis**: Scripts for data processing, visualization, and statistical analysis
- **Reproducible methodology**: Documented approach for extracting and analyzing data
- **Visual synthesis**: Charts, graphs, and tables that reveal new insights from existing data

**Concrete Examples of Data-Driven Contributions:**

**Software Section:**
- Comparative performance tables from HumanEval, MBPP, SWE-bench results across LLM code generators
- Timeline analysis of compiler optimization techniques adoption (2020-2025)
- GPU kernel performance comparison matrices from published benchmarks

**Architecture Section:**
- Design space exploration visualizations aggregated from accelerator papers
- Memory system performance trend analysis from ISCA/MICRO papers
- Energy-performance Pareto frontier analysis across AI accelerators

**Chip Design Section:**
- EDA tool capability comparison matrix (features, supported designs, performance)
- Synthesis QoR improvement trends over time from published results
- Verification coverage analysis across different AI-assisted approaches

**Integration Section:**
- Cross-stack methodology taxonomy with quantitative adoption analysis
- Timeline of end-to-end design flow evolution
- Gap analysis matrix showing unexplored cross-layer optimization opportunities

**The key insight**: Students extract, synthesize, and visualize data that already exists in the literature but hasn't been systematically compared or analyzed across the full stack.

### Three-Checkpoint System

{: .checkpoint-list}
#### üìã Checkpoint 1: Foundation & Technical Planning (October 1)
**Who**: All students working on software-related sections  
**Deliverable**: Literature foundation + technical work proposal (4-5 pages)  
**Requirements**:
- 20-25 key papers identified with data extraction plan
- Cross-cutting themes and quantitative analysis opportunities identified
- Detailed plan for tables/figures to be generated from existing data
- Methodology for data collection and analysis documented

{: .checkpoint-list}
#### üìä Checkpoint 2: Technical Results & Draft Analysis (October 29)
**Who**: All students working on architecture-related sections  
**Deliverable**: Technical results + draft survey sections (10-12 pages)  
**Requirements**:
- Completed data analysis with generated tables and figures
- Comparative analysis revealing new insights from existing literature
- Draft survey sections with quantitative findings integrated
- Cross-references and data connections to other sections identified

{: .checkpoint-list}
#### üìñ Checkpoint 3: Integrated Survey Paper (November 19)
**Who**: All students working on chip design & integration sections  
**Deliverable**: Complete integrated survey paper (18-22 pages)  
**Requirements**:
- Comprehensive cross-stack analysis with data-driven insights
- Quantitative analysis seamlessly integrated into narrative
- Cross-cutting themes supported by comparative data and visualizations
- Future research directions based on identified gaps and trends
- Publication-ready quality with reproducible analysis methodology

{: .checkpoint-list}
#### üéØ Final Presentations (December 1)
**Who**: All groups  
**Deliverable**: 15-minute presentation + Q&A  
**Requirements**:
- Clear communication of survey findings
- Novel insights and research contributions
- Future directions and open problems
- Professional presentation quality

### Execution Process

**Section Assignment (Week 2):**
- Students rank preferences for survey sections (1-5)
- Instructor assigns students balancing technical backgrounds and interests
- Teams of 2-3 students per section for comprehensive coverage

**Technical Work Planning Meeting (Week 3):**
- 45-minute meeting with instructor and TAs per team
- Discuss section scope, cross-cutting themes, and technical work plans
- Approve programming/analysis projects and methodologies
- Establish individual technical responsibilities and integration strategy

**Cross-Team Coordination:**
- **Weekly integration meetings** to ensure coherent narrative
- **Shared technical infrastructure** (code repositories, data formats)
- **Cross-referencing protocols** to highlight connections between sections

**Ongoing Support:**
- **Bi-weekly check-ins** with TAs during office hours
- **Peer review sessions** between groups working on related topics
- **Cross-group presentations** to share insights and get feedback
- **Mandatory progress updates** at each checkpoint

### Assessment Rubric

**Data-Driven Analysis (40%)**
- Quality and insight of generated tables/figures (15%)
- Rigor of data extraction and analysis methodology (15%)
- Integration of quantitative findings with survey narrative (10%)

**Survey Writing (35%)**
- Literature comprehension and synthesis (15%)
- Cross-cutting analysis and novel insights (10%)
- Writing quality and technical communication (10%)

**Collaboration & Integration (25%)**
- Cross-team coordination and referencing (10%)
- Checkpoint deliverable quality (10%)
- Final presentation and Q&A (5%)

**Quality Assurance Mechanisms:**
- **Data analysis review sessions** with TAs to validate methodology
- **Cross-team presentations** of key findings and visualizations
- **Faculty review of analytical contributions** before survey integration
- **External reviewer feedback** on final paper before submission

### Survey Paper Standards

**Analytical Rigor:**
- **Data-Driven Insights**: All major claims supported by tables, figures, or quantitative analysis
- **Comparative Analysis**: Systematic comparisons revealing trends and gaps across literature
- **Cross-Stack Synthesis**: Quantitative findings that connect insights across abstraction levels

**Survey Quality:**
- **Comprehensive Coverage**: Systematic review of 100+ papers across all areas (2020-2025)
- **Novel Cross-Cutting Analysis**: Identification of methodological connections across stack
- **Original Technical Insights**: New benchmarking results, comparative analysis, or unified frameworks
- **Rich Visualizations**: Technical diagrams, performance comparisons, and taxonomy figures
- **Actionable Future Directions**: 5-7 concrete research opportunities with technical justification

**Publication Standards:**
- **Venue Target**: ISCA, MICRO, Computer Architecture Letters, or ACM Computing Surveys
- **Length**: 18-22 pages with comprehensive technical appendix
- **Reproducibility Package**: Complete code repository with documentation and datasets

---

## Course Policies

### Late Work
- Survey paper checkpoints: 10% penalty per day late
- Survey paper project: Extensions require advance notice from entire team
- Discussion leadership: Must reschedule in advance

### Collaboration
- **Survey paper checkpoints**: Individual work within collaborative project framework
- **Survey paper project**: Required group work (entire class collaboration)
- **Discussion leadership**: Pairs allowed and encouraged

---

